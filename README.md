# MCA Insights Engine ðŸš€

This project implements the MCA Insights Engine assignment, processing Ministry of Corporate Affairs (MCA) data to detect changes, enrich records, and provide an interactive dashboard.

## Setup Instructions

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/Alok594/Mca-insights-engine.git](https://github.com/YOUR_USERNAME/Mca-insights-engine.git)
    cd Mca-insights-engine
    ```
2.  **Create a Python virtual environment:**
    ```bash
    python -m venv venv
    ```
3.  **Activate the environment:**
    * Windows: `.\venv\Scripts\activate`
4.  **Install required libraries:**
    ```bash
    pip install -r requirements.txt
    ```

## How to Run

1.  **Download Raw Data (Not Included):**
    * The large raw state CSV files (`delhi.csv`, `G.csv`, `K.csv`, `mh.csv`, `tamil.csv`) are **not** included in this repository due to GitHub file size limits.
    * Please download them from the **Open Government Data Platform India (`data.gov.in`)** by searching for "Company Master Data" and selecting the "Registrars of Companies (RoC)-wise Company Master Data" resource.
    * Place the downloaded CSV files into the main project folder (`Mca-insights-engine`).

2.  **Generate Processed Data & Logs:**
    * Run the data processing script:
        ```bash
        python process_data.py
        ```
    * This will create the `master_dataset.csv`, `change_log_day2.json`, `change_log_day3.json`, and `daily_summary.txt` files in the `output` folder using the raw data and the included `day1.csv`, `day2.csv`, `day3.csv` simulation files.

3.  **Run the Streamlit Dashboard:**
    ```bash
    streamlit run app.py
    ```
    * The application will open in your web browser (usually at `http://localhost:8501`).

## Project Architecture

* **`process_data.py`**: Handles Task A (Data Integration), Task B (Change Detection), Task E (Part 1 - AI Summary), and Task C (Web Enrichment attempt). It reads the raw state CSVs, cleans them, compares daily snapshots (`day1.csv`, etc.), generates change logs and summaries, and attempts web scraping.
* **`app.py`**: Implements the Streamlit dashboard (Task D) and the Chatbot (Task E Part 2). It loads the processed data, provides filters, search functionality, detailed views, and the chat interface.
* **`output/`**: Contains all generated files (master dataset, change logs, summary, enriched data).
* **`requirements.txt`**: Lists all Python dependencies.
* **`.gitignore`**: Specifies files and folders for Git to ignore (like `venv/` and `*.csv`).
* **`README.md`**: This file.

## Enrichment Logic (Task C)

* The script attempts to scrape director names, email, and website information from ZaubaCorp for a sample of CINs found in the change logs.
* **Note:** The automated web scraping frequently fails due to `403 Forbidden` errors from ZaubaCorp's anti-scraping measures. As per the assignment guidelines (allowing a "proxy implementation"), a sample `enriched_company_data.csv` file has been manually created and included in the `output` folder to demonstrate the intended structure and allow the dashboard (Task D) to function correctly.

## Deliverables Check

* [x] Source code (GitHub): This repository (excluding large raw CSVs).
* [x] Processed Dataset: `output/master_dataset.csv` (generated by script)
* [x] Change Logs: `output/change_log_day2.json`, `output/change_log_day3.json` (generated by script)
* [x] Enriched Dataset: `output/enriched_company_data.csv` (Manually created proxy)
* [x] AI Summary: `output/daily_summary.txt` (generated by script)
* [x] Dashboard: `app.py`
* [x] README: This file.
